<?xml version="1.0"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->
<configuration>
    <!--basic -->
    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.zk-base-path</name>
        <value>/hadoop/yarn-mrcluster-leader-election</value>
    </property>
    <property>
        <name>hadoop.zk.address</name>
        <value>VIP-DC-MASTER-1:2181,VIP-DC-MASTER-2:2181,VIP-DC-MASTER-3:2181</value>
    </property>
    <!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.ha.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.cluster-id</name>
        <value>yarn-ha</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.rm-ids</name>
        <value>rm1,rm2</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm1</name>
        <value>VIP-DC-MASTER-1</value>
    </property>
    <property>
        <name>yarn.resourcemanager.hostname.rm2</name>
        <value>VIP-DC-MASTER-2</value>
    </property>
    <!--在master上配置rm1,在slave1上配置rm2, 其它slave节点不用配这个属性-->
    <property>
        <name>yarn.resourcemanager.ha.id</name>
        <value>rm1</value>
        <description>If we want to launch more than one RM in single node,we need this configuration</description>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle,spark_shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.spark_shuffle.class</name>
      <value>org.apache.spark.network.yarn.YarnShuffleService</value>
    </property>
    <property>
      <name>spark.shuffle.service.port</name>
      <value>7337</value>
    </property>
    <!-- <property>
        <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property> -->
    <!-- <property>
        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
        <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property> -->
    <property>
        <name>yarn.resourcemanager.connect.retry-interval.ms</name>
        <value>2000</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-state-store.address</name>
        <value>VIP-DC-MASTER-1,VIP-DC-MASTER-2,VIP-DC-MASTER-3</value>
    </property>
    <property>
        <name>yarn.resourcemanager.store.class</name>
        <value>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore</value>
    </property>
    <property>
        <name>yarn.resourcemanager.zk-address</name>
        <value>VIP-DC-MASTER-1:2181,VIP-DC-MASTER-2:2181,VIP-DC-MASTER-3:2181</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.scheduler.connection.wait.interval-ms</name>
        <value>5000</value>
    </property>
    <!--配置rm1-->
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm1</name>
        <value>VIP-DC-MASTER-1:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm1</name>
        <value>VIP-DC-MASTER-1:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address.rm1</name>
        <value>VIP-DC-MASTER-1:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address.rm1</name>
        <value>VIP-DC-MASTER-1:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm1</name>
        <value>VIP-DC-MASTER-1:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.admin.address.rm1</name>
        <value>VIP-DC-MASTER-1:23142</value>
    </property>
    <!--配置rm2-->
    <property>
        <name>yarn.resourcemanager.scheduler.address.rm2</name>
        <value>VIP-DC-MASTER-2:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address.rm2</name>
        <value>VIP-DC-MASTER-2:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address.rm2</name>
        <value>VIP-DC-MASTER-2:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address.rm2</name>
        <value>VIP-DC-MASTER-2:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address.rm2</name>
        <value>VIP-DC-MASTER-2:8088</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.admin.address.rm2</name>
        <value>VIP-DC-MASTER-2:23142</value>
    </property>
    <!-- modify schduler config  -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>20480</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>5</value>
    </property>
    <!--nodemanager 配置-->
    <property>
        <name>yarn.nodemanager.recovery.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>yarn.nodemanager.address</name>
        <value>0.0.0.0:8041</value>
    </property>
    <!-- <property>
        <name>yarn.nodemanager.recovery.dir</name>
        <value>${hadoop.data.dir}/yarn-nm-recovery</value>
    </property> -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>230400</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>56</value>
        <description>Number of vcores that can be allocated for containers.
            This is used by the RM scheduler when allocating resources for
            containers. This is not used to limit the number of physical cores
            used by YARN containers.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/devdata/disk/dir6/spark-tmp</value>
        <!-- <value>/devdata/temp/haddop/local</value> -->
    </property>
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>/devdata/log/haddop/containers</value>
    </property>
    <property>
        <name>mapreduce.shuffle.port</name>
        <value>23080</value>
    </property>
    <property>
        <name>yarn.client.failover-proxy-provider</name>
        <value>org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider</value>
    </property>
    <property>
        <name>yarn.resourcemanager.ha.automatic-failover.zk-base-path</name>
        <value>/hadoop/yarn-leader-election</value>
        <description>Optionalsetting.Thedefaultvalueis/yarn-leader-election</description>
    </property>
    <property>
        <name>yarn.nodemanager.disk-health-checker.enable</name>
        <value>false</value>
    </property>
    <!--appLog -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
        <description>Whether to enable log aggregation or not. If disabled,
            NMs will keep the logs
            locally (like in 1.x) and not aggregate them
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>/devdata/log/haddop/apps</value>
        <description>NOTICE:
            <!-- hdfs path need use full path ,otherwise , the path is current 
                user sub dirs. -->
            This is on the default file-system, usually HDFS and indictes where
            the NMs should aggregate logs to. This should not be local
            file-system, otherwise
            serving daemons like history-server will not
            able to serve the aggregated
            logs.
            Default is /tmp/logs
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
        <value>logs</value>
        <description>The remote log dir will be created at
            {yarn.nodemanager.remote-app-log-dir}/${user}/{thisParam}. Default
            value is “logs””.
        </description>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
        <description>604800 = 7 * 24 * 60 *60 s, one week.Default is -1,How
            long to wait before deleting
            aggregated-logs, -1 or a negative number
            disables the deletion of
            aggregated-logs. One needs to be careful and
            not set this to a too
            small a value so as to not burden the
            distributed file-system.
        </description>
    </property>
    <property>
        <name>yarn.log-aggregation.retain-check-interval-seconds</name>
        <value>-1</value>
        <description>Determines how long to wait between aggregated-log
            retention-checks. If it is set to 0 or a negative value, then the
            value is computed as one-tenth of the aggregated-log retention-time.
            As with the previous configuration property, one needs to be careful
            and not set this to low values. Defaults to -1.
        </description>
    </property>
    <property>
        <name>yarn.nodemanager.container-monitor.interval-ms</name>
        <value>10000</value>
    </property>
    <property>
        <name>yarn.log.server.url</name>
        <value>http://VIP-DC-MASTER-1:19888/jobhistory/logs</value>
    </property>
    <!-- <property>
        <name>yarn.web-proxy.address</name>
        <value>VIP-DC-MASTER-1:29000</value>
        <description>The address for the web proxy as HOST:PORT, if this is
            not given then the proxy will run as part of the RM
        </description>
    </property> -->
    <property>
        <name>yarn.app.mapreduce.am.job.client.port-range</name>
        <value>40000-50000</value>
    </property>

        <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认
  是 true -->
  <property>
    <name>yarn.nodemanager.pmem-check-enabled</name>
    <value>false</value>
  </property>
  
    <!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认
    是 true -->
  <property>
    <name>yarn.nodemanager.vmem-check-enabled</name>
    <value>false</value>
  </property>

</configuration>
